{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64501211",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64501211",
        "outputId": "8ec7012e-1816-442b-fb66-f99c547f722e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "86137231",
      "metadata": {
        "id": "86137231"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy.signal import find_peaks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy.fft import rfft\n",
        "from statsmodels.tsa.stattools import acf\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "45aa7f27",
      "metadata": {
        "id": "45aa7f27"
      },
      "outputs": [],
      "source": [
        "# --- 1. Simulation Function ---\n",
        "def simulate_black_hole_lightcurve(fs, fc, fm, qpo_amplitude, duration,\n",
        "                                   noise_mean=0, noise_std=0.5,\n",
        "                                   include_qpo=True, modulation_index=0.5):\n",
        "    \"\"\"\n",
        "    Simulate a black hole light curve with stochastic noise and an amplitude-modulated QPO signal.\n",
        "\n",
        "    Parameters:\n",
        "        fs : int\n",
        "            Sampling frequency (Hz)\n",
        "        fc : float\n",
        "            Carrier frequency (Hz) for QPO\n",
        "        fm : float\n",
        "            Modulating frequency (Hz) for QPO\n",
        "        qpo_amplitude : float\n",
        "            Amplitude of the carrier signal (QPO)\n",
        "        duration : float\n",
        "            Duration of lightcurve (seconds)\n",
        "        noise_mean : float\n",
        "            Mean of the Gaussian noise\n",
        "        noise_std : float\n",
        "            Standard deviation of the Gaussian noise\n",
        "        include_qpo : bool\n",
        "            Whether to include the QPO signal\n",
        "        modulation_index : float\n",
        "            Modulation index for AM signal\n",
        "\n",
        "    Returns:\n",
        "        t : np.ndarray\n",
        "            Time array\n",
        "        flux : np.ndarray\n",
        "            Normalized flux array\n",
        "    \"\"\"\n",
        "    # Time array\n",
        "    t = np.arange(0, duration, 1/fs)\n",
        "\n",
        "    # White noise\n",
        "    white_noise = np.random.normal(noise_mean, noise_std, size=len(t))\n",
        "    white_noise = np.exp(white_noise)\n",
        "\n",
        "    if include_qpo and qpo_amplitude > 0:\n",
        "        # Modulating signal\n",
        "        msg = qpo_amplitude * np.cos(2 * np.pi * fm * t)\n",
        "\n",
        "        # Carrier signal\n",
        "        carrier = qpo_amplitude * np.cos(2 * np.pi * fc * t)\n",
        "\n",
        "        # AM QPO signal\n",
        "        qpo = carrier * (1 + modulation_index * msg / qpo_amplitude)\n",
        "    else:\n",
        "        qpo = 0\n",
        "\n",
        "    # Combine noise and QPO signal\n",
        "    flux = white_noise + qpo\n",
        "\n",
        "    # Normalize\n",
        "    flux = (flux - np.mean(flux)) / np.std(flux)\n",
        "\n",
        "    return t, flux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce020a18",
      "metadata": {
        "id": "ce020a18"
      },
      "outputs": [],
      "source": [
        "def generate_dataset_with_fc_amp(output_dir, num_samples=5000,\n",
        "                                 fs=1, duration=512,\n",
        "                                 modulation_index=0.5,\n",
        "                                 amp_range=(0.1, 1.0)):\n",
        "    \"\"\"\n",
        "    Generate a dataset of light curves with fc and amp labels for conditional GAN/SBI.\n",
        "\n",
        "    Stores:\n",
        "        X: (num_samples, 512, 1) - light curves\n",
        "        y: (num_samples, 3) - [fc, amp, is_qpo]\n",
        "\n",
        "    Parameters:\n",
        "        output_dir : str\n",
        "            Path to save dataset\n",
        "        num_samples : int\n",
        "            Total number of samples (half QPO, half non-QPO)\n",
        "        fs : int\n",
        "            Sampling frequency\n",
        "        duration : int\n",
        "            Light curve length in seconds\n",
        "        modulation_index : float\n",
        "            AM index for QPO\n",
        "        amp_range : tuple\n",
        "            Amplitude range (min, max)\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    seq_length = int(duration * fs)\n",
        "    X, y = [], []\n",
        "\n",
        "    for _ in range(num_samples // 2):\n",
        "        fc = np.random.uniform(0.01, 0.5)\n",
        "        fm = np.random.uniform(0.005, 0.1)\n",
        "        amp = np.random.uniform(*amp_range)\n",
        "\n",
        "        # --- QPO light curve ---\n",
        "        t, flux_qpo = simulate_black_hole_lightcurve(\n",
        "            fs, fc, fm, amp, duration, include_qpo=True,\n",
        "            modulation_index=modulation_index\n",
        "        )\n",
        "        X.append(flux_qpo[:seq_length].reshape(-1, 1))\n",
        "        y.append([fc, amp, 1])  # label: 1 = QPO\n",
        "\n",
        "        # --- Non-QPO light curve ---\n",
        "        _, flux_non_qpo = simulate_black_hole_lightcurve(\n",
        "            fs, fc, fm, amp, duration, include_qpo=False,\n",
        "            modulation_index=modulation_index\n",
        "        )\n",
        "        X.append(flux_non_qpo[:seq_length].reshape(-1, 1))\n",
        "        y.append([fc, amp, 0])  # label: 0 = non-QPO\n",
        "\n",
        "    # Convert and save\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.float32)\n",
        "\n",
        "    np.savez_compressed(os.path.join(output_dir, \"data_fc_amp.npz\"), X=X, y=y)\n",
        "    print(\n",
        "        f\" Saved dataset to {output_dir}/data_fc_amp.npz with shape X: {X.shape}, y: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b7e6534",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b7e6534",
        "outputId": "40dfe766-d201-4a23-93c3-62b22eb1cbab"
      },
      "outputs": [],
      "source": [
        "output_folder = \"qpo_physical_dataset/dataset_fc_amp\"\n",
        "\n",
        "generate_dataset_with_fc_amp(\n",
        "    output_dir=output_folder,\n",
        "    num_samples=10000,\n",
        "    amp_range=(0.1, 1.0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3832662",
      "metadata": {
        "id": "f3832662"
      },
      "outputs": [],
      "source": [
        "def load_physical_conditional_dataset(npz_path, batch_size=64, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Load dataset with labels = [fc, amp, is_qpo]\n",
        "    Returns tf.data.Dataset of (X, [fc, amp, is_qpo])\n",
        "    \"\"\"\n",
        "    data = np.load(npz_path)\n",
        "    X = data[\"X\"].astype(np.float32)         # shape: (N, 512, 1)\n",
        "    y = data[\"y\"].astype(np.float32)         # shape: (N, 3)\n",
        "\n",
        "    # Split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Make tf.data datasets\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "        (X_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices(\n",
        "        (X_val, y_val)).batch(batch_size)\n",
        "\n",
        "    return train_ds, val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1cfa9ca6",
      "metadata": {
        "id": "1cfa9ca6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 89\u001b[0m\n\u001b[1;32m     84\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel([series_input, condition_input], output)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConditionalTimeSeriesGAN\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel):\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    Conditional GAN for time series generation with QPOs.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    Implements the training loop for both generator and discriminator.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, generator, discriminator, latent_dim, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "def build_full_conditional_generator(latent_dim=100, condition_dim=3, seq_length=512):\n",
        "    \"\"\"\n",
        "    Builds a conditional generator for time series data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    latent_dim : int, default=100\n",
        "        Dimension of the noise input vector\n",
        "    condition_dim : int, default=3\n",
        "        Dimension of the condition vector (fc, amp, is_qpo)\n",
        "    seq_length : int, default=512\n",
        "        Length of the output time series\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tf.keras.Model\n",
        "        Conditional generator model\n",
        "    \"\"\"\n",
        "    # Input layers\n",
        "    noise_input = tf.keras.Input(shape=(latent_dim,))\n",
        "    condition_input = tf.keras.Input(shape=(condition_dim,))  # fc, amp, is_qpo\n",
        "\n",
        "    # Combine noise and condition vectors\n",
        "    x = tf.keras.layers.Concatenate()([noise_input, condition_input])\n",
        "\n",
        "    # Initial dense layers for processing combined input\n",
        "    x = tf.keras.layers.Dense(128)(x)\n",
        "    x = tf.keras.layers.LeakyReLU()(x)  # Non-linear activation\n",
        "\n",
        "    # Project to sequence length\n",
        "    x = tf.keras.layers.Dense(seq_length * 32)(x)  # 32 features per time step\n",
        "    x = tf.keras.layers.Reshape((seq_length, 32))(\n",
        "        x)  # Reshape to time series format\n",
        "\n",
        "    # Add noise for regularization\n",
        "    x = tf.keras.layers.GaussianNoise(0.05)(x)\n",
        "\n",
        "    # GRU layer for temporal coherence\n",
        "    x = tf.keras.layers.GRU(64, return_sequences=True)(x)\n",
        "\n",
        "    # Output layer: one value per time step\n",
        "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))(x)\n",
        "\n",
        "    return tf.keras.Model([noise_input, condition_input], output)\n",
        "\n",
        "\n",
        "def build_full_conditional_discriminator(seq_length=512, condition_dim=3):\n",
        "    \"\"\"\n",
        "    Builds a conditional discriminator for time series data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    seq_length : int, default=512\n",
        "        Length of the input time series\n",
        "    condition_dim : int, default=3\n",
        "        Dimension of the condition vector (fc, amp, is_qpo)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tf.keras.Model\n",
        "        Conditional discriminator model\n",
        "    \"\"\"\n",
        "    # Input layers\n",
        "    series_input = tf.keras.Input(shape=(seq_length, 1))\n",
        "    condition_input = tf.keras.Input(shape=(condition_dim,))  # fc, amp, is_qpo\n",
        "\n",
        "    # Expand conditions to match sequence length\n",
        "    condition_expanded = tf.keras.layers.RepeatVector(\n",
        "        seq_length)(condition_input)  # Repeat conditions for each time step\n",
        "\n",
        "    # Combine time series with conditions\n",
        "    x = tf.keras.layers.Concatenate()([series_input, condition_expanded])\n",
        "\n",
        "    # GRU layers for temporal processing\n",
        "    x = tf.keras.layers.GRU(64, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.GRU(32)(x)  # Final GRU layer returns single vector\n",
        "\n",
        "    # Dense layers for classification\n",
        "    x = tf.keras.layers.Dense(64)(x)\n",
        "    x = tf.keras.layers.LeakyReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)  # Regularization\n",
        "\n",
        "    # Output layer: single value for real/fake classification\n",
        "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return tf.keras.Model([series_input, condition_input], output)\n",
        "\n",
        "\n",
        "class ConditionalTimeSeriesGAN(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Conditional GAN for time series generation with QPOs.\n",
        "    Implements the training loop for both generator and discriminator.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, generator, discriminator, latent_dim, learning_rate=1e-4):\n",
        "        \"\"\"\n",
        "        Initialize the GAN with its components.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        generator : tf.keras.Model\n",
        "            The generator model\n",
        "        discriminator : tf.keras.Model\n",
        "            The discriminator model\n",
        "        latent_dim : int\n",
        "            Dimension of the noise input vector\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "        # Small learning rate for stable training\n",
        "        self.gen_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "        self.disc_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "    def compile(self):\n",
        "        super().compile()\n",
        "\n",
        "    @tf.function  # Compile function for faster execution\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Single training step for both generator and discriminator.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : tuple\n",
        "            Contains (real_series, labels)\n",
        "        \"\"\"\n",
        "        real_series, labels = data\n",
        "        batch_size = tf.shape(real_series)[0]\n",
        "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            # Generate fake samples\n",
        "            fake_series = self.generator([noise, labels], training=True)\n",
        "\n",
        "            # Get discriminator outputs for real and fake samples\n",
        "            real_output = self.discriminator(\n",
        "                [real_series, labels], training=True)\n",
        "            fake_output = self.discriminator(\n",
        "                [fake_series, labels], training=True)\n",
        "\n",
        "            # Calculate discriminator loss\n",
        "            real_labels = tf.ones((batch_size, 1)) * 0.9  # Label smoothing\n",
        "            fake_labels = tf.zeros((batch_size, 1))\n",
        "            disc_loss = self.loss_fn(\n",
        "                real_labels, real_output) + self.loss_fn(fake_labels, fake_output)\n",
        "\n",
        "            # Extra penalty for non-QPO samples in frequency domain\n",
        "            is_non_qpo = tf.cast(tf.equal(labels[:, 2], 0.0), tf.bool)\n",
        "            non_qpo_fake = tf.boolean_mask(fake_series, is_non_qpo)\n",
        "\n",
        "            def simple_psd(x):\n",
        "                \"\"\"Calculate power spectral density\"\"\"\n",
        "                x = tf.squeeze(x, axis=-1)\n",
        "                x_fft = tf.signal.rfft(x)\n",
        "                return tf.abs(x_fft)\n",
        "\n",
        "            # Add frequency penalty for non-QPO samples\n",
        "            if tf.shape(non_qpo_fake)[0] > 0:\n",
        "                psd_vals = simple_psd(non_qpo_fake)\n",
        "                max_peaks = tf.reduce_max(psd_vals, axis=-1)\n",
        "                freq_penalty = tf.reduce_mean(max_peaks)\n",
        "                disc_loss += 0.05 * freq_penalty  # Adjustable weight\n",
        "\n",
        "        # Update discriminator weights\n",
        "        grads_disc = disc_tape.gradient(\n",
        "            disc_loss, self.discriminator.trainable_variables)\n",
        "        self.disc_optimizer.apply_gradients(\n",
        "            zip(grads_disc, self.discriminator.trainable_variables))\n",
        "\n",
        "        # === Train Generator ===\n",
        "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            # Generate new fake samples\n",
        "            generated_series = self.generator([noise, labels], training=True)\n",
        "            fake_output = self.discriminator(\n",
        "                [generated_series, labels], training=True)\n",
        "            # Generator tries to fool discriminator\n",
        "            gen_loss = self.loss_fn(tf.ones((batch_size, 1)), fake_output)\n",
        "\n",
        "        # Update generator weights\n",
        "        grads_gen = gen_tape.gradient(\n",
        "            gen_loss, self.generator.trainable_variables)\n",
        "        self.gen_optimizer.apply_gradients(\n",
        "            zip(grads_gen, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"gen_loss\": gen_loss, \"disc_loss\": disc_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b288eb",
      "metadata": {
        "id": "56b288eb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import welch\n",
        "from statsmodels.tsa.stattools import acf\n",
        "from scipy.stats import lognorm\n",
        "\n",
        "\n",
        "def plot_psd(series, fs=1, label='PSD'):\n",
        "    f, Pxx = welch(series, fs=fs, nperseg=256)\n",
        "    plt.semilogy(f, Pxx, label=label)\n",
        "    plt.xlabel(\"Frequency (Hz)\")\n",
        "    plt.ylabel(\"Power\")\n",
        "    plt.title(\"Power Spectral Density\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def plot_acf(series, lags=100, label='ACF'):\n",
        "    autocorr = acf(series, nlags=lags, fft=True)\n",
        "    plt.plot(autocorr, label=label)\n",
        "    plt.xlabel(\"Lag\")\n",
        "    plt.ylabel(\"Autocorrelation\")\n",
        "    plt.title(\"ACF\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def plot_flux_histogram(series, label='Generated'):\n",
        "    shape, loc, scale = lognorm.fit(series - np.min(series) + 1e-6)\n",
        "    x = np.linspace(np.min(series), np.max(series), 100)\n",
        "    pdf = lognorm.pdf(x, shape, loc, scale)\n",
        "\n",
        "    plt.hist(series, bins=40, density=True, alpha=0.6, label=f\"{label} Flux\")\n",
        "    plt.plot(x, pdf, '--', label=f\"LogNorm Fit ({label})\")\n",
        "    plt.xlabel(\"Flux\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.title(\"Flux Histogram with Log-normal Fit\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def compute_qpo_score(series):\n",
        "    from scipy.signal import welch\n",
        "    from scipy.optimize import curve_fit\n",
        "\n",
        "    def lorentz(f, A, f0, gamma):\n",
        "        return A / (1 + ((f - f0)/gamma)**2)\n",
        "\n",
        "    f, Pxx = welch(series, fs=1, nperseg=256)\n",
        "    mask = (f > 0.01) & (f < 0.45)\n",
        "    try:\n",
        "        p0 = [np.max(Pxx[mask]), f[mask][np.argmax(Pxx[mask])], 0.01]\n",
        "        popt, _ = curve_fit(lorentz, f[mask], Pxx[mask], p0=p0, maxfev=2000)\n",
        "        Q = popt[1] / popt[2]\n",
        "        return float(Q)\n",
        "    except:\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8fd63da",
      "metadata": {
        "id": "c8fd63da"
      },
      "outputs": [],
      "source": [
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import csv\n",
        "\n",
        "\n",
        "class GANMonitor(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, generator, val_dataset, latent_dim, num_samples=3, label=[0, 1], save_dir=\"gan_outputs\", csv_log_path=\"saved_models/qpo_scores.csv\"):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "        self.val_dataset = val_dataset\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_samples = num_samples\n",
        "        self.label = tf.convert_to_tensor(\n",
        "            [label] * num_samples, dtype=tf.float32)\n",
        "        self.save_dir = save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        self.csv_log_path = csv_log_path\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        #  Create CSV file header\n",
        "        with open(self.csv_log_path, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"epoch\", \"sample_q_score\"])\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        noise = tf.random.normal([self.num_samples, self.latent_dim])\n",
        "        generated = self.generator(\n",
        "            [noise, self.label], training=False).numpy().squeeze()\n",
        "        real_samples = next(iter(self.val_dataset))[\n",
        "            0].numpy().squeeze()[:self.num_samples]\n",
        "\n",
        "        pdf_path = os.path.join(self.save_dir, f\"epoch_{epoch+1}.pdf\")\n",
        "        with PdfPages(pdf_path) as pdf:\n",
        "            for i in range(self.num_samples):\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "                plt.sca(axes[0])\n",
        "                plot_psd(generated[i], label='Generated')\n",
        "                plot_psd(real_samples[i], label='Real')\n",
        "\n",
        "                plt.sca(axes[1])\n",
        "                plot_acf(generated[i], label='Generated')\n",
        "                plot_acf(real_samples[i], label='Real')\n",
        "\n",
        "                plt.sca(axes[2])\n",
        "                plot_flux_histogram(generated[i], label='Generated')\n",
        "                plot_flux_histogram(real_samples[i], label='Real')\n",
        "\n",
        "                plt.suptitle(f\"Sample {i + 1} – Epoch {epoch + 1}\")\n",
        "                plt.tight_layout()\n",
        "\n",
        "                #  Save this figure as one page in the PDF\n",
        "                pdf.savefig(fig)\n",
        "                plt.show(fig)\n",
        "                plt.close(fig)\n",
        "        q_score = compute_qpo_score(generated[0])\n",
        "        print(f\"Epoch {epoch+1} – Sample Q: {q_score}\")\n",
        "        print(f\"Saved PDF for epoch {epoch + 1} at: {pdf_path}\")\n",
        "        #  Save to CSV\n",
        "\n",
        "        with open(self.csv_log_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([epoch + 1, q_score])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc01f7c",
      "metadata": {
        "id": "efc01f7c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "# csv_logger = tf.keras.callbacks.CSVLogger(\"saved_models/conditional_gan_training_log.csv\", append=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b2337cc",
      "metadata": {
        "id": "2b2337cc"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "class ValidationLossLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, val_dataset, latent_dim, generator, discriminator, save_dir=\"saved_models\", csv_path=\"saved_models/cgan_phy_log_with_val.csv\"):\n",
        "        super().__init__()\n",
        "        self.val_dataset = val_dataset\n",
        "        self.latent_dim = latent_dim\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "        self.csv_path = csv_path\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"epoch\", \"gen_loss\", \"disc_loss\",\n",
        "                            \"val_gen_loss\", \"val_disc_loss\"])\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_gen_loss_total = 0.0\n",
        "        val_disc_loss_total = 0.0\n",
        "        batches = 0\n",
        "\n",
        "        for real_series, labels in self.val_dataset:\n",
        "            batch_size = tf.shape(real_series)[0]\n",
        "            noise = tf.random.normal([batch_size, self.latent_dim])\n",
        "            fake_series = self.generator([noise, labels], training=False)\n",
        "\n",
        "            # Discriminator loss on validation data\n",
        "            real_output = self.discriminator(\n",
        "                [real_series, labels], training=False)\n",
        "            fake_output = self.discriminator(\n",
        "                [fake_series, labels], training=False)\n",
        "            real_labels = tf.ones((batch_size, 1)) * 0.9\n",
        "            fake_labels = tf.zeros((batch_size, 1))\n",
        "            disc_loss = self.loss_fn(\n",
        "                real_labels, real_output) + self.loss_fn(fake_labels, fake_output)\n",
        "\n",
        "            # Generator loss on validation data\n",
        "            fake_output = self.discriminator(\n",
        "                [fake_series, labels], training=False)\n",
        "            gen_loss = self.loss_fn(tf.ones((batch_size, 1)), fake_output)\n",
        "\n",
        "            val_gen_loss_total += gen_loss.numpy()\n",
        "            val_disc_loss_total += disc_loss.numpy()\n",
        "            batches += 1\n",
        "\n",
        "        avg_gen = val_gen_loss_total / batches\n",
        "        avg_disc = val_disc_loss_total / batches\n",
        "\n",
        "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                epoch + 1,\n",
        "                logs[\"gen_loss\"],\n",
        "                logs[\"disc_loss\"],\n",
        "                avg_gen,\n",
        "                avg_disc\n",
        "            ])\n",
        "        print(\n",
        "            f\" Logged validation losses (gen: {avg_gen:.4f}, disc: {avg_disc:.4f})\")\n",
        "        # Save models on each epoch\n",
        "        self.generator.save(os.path.join(\n",
        "            self.save_dir, f\"generator_{epoch + 1}.keras\"))\n",
        "        self.discriminator.save(os.path.join(\n",
        "            self.save_dir, f\"discriminator_{epoch + 1}.keras\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "106eb9b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "106eb9b3",
        "outputId": "5ac30d23-86ac-46a4-ecf4-97fdf7670cdf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# === Hyperparameter Grid ===\n",
        "latent_dims = [64, 100]\n",
        "learning_rates = [1e-3, 5e-4]\n",
        "batch_sizes = [64, 128]\n",
        "\n",
        "EPOCHS = 100\n",
        "DATA_PATH = \"qpo_physical_dataset/dataset_fc_amp/data_fc_amp.npz\"\n",
        "OUTPUT_DIR = \"cgan_hyperparam_runs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# === Loop Over Hyperparameters ===\n",
        "experiment_id = 0\n",
        "for latent_dim, lr, batch_size in itertools.product(latent_dims, learning_rates, batch_sizes):\n",
        "    experiment_id += 1\n",
        "    config_name = f\"run_{experiment_id}_z{latent_dim}_lr{lr}_bs{batch_size}\"\n",
        "    run_path = os.path.join(OUTPUT_DIR, config_name)\n",
        "    os.makedirs(run_path, exist_ok=True)\n",
        "    print(f\"\\nStarting {config_name}...\\n\")\n",
        "\n",
        "    # Load data\n",
        "    train_ds, val_ds = load_physical_conditional_dataset(\n",
        "        DATA_PATH, batch_size=batch_size)\n",
        "\n",
        "    # Build models\n",
        "    generator = build_full_conditional_generator(latent_dim=latent_dim)\n",
        "    discriminator = build_full_conditional_discriminator()\n",
        "\n",
        "    # Compile GAN\n",
        "    gan = ConditionalTimeSeriesGAN(\n",
        "        generator, discriminator, latent_dim, lr)\n",
        "    gan.compile()\n",
        "\n",
        "    # Callbacks\n",
        "    monitor = GANMonitor(\n",
        "        generator=generator,\n",
        "        val_dataset=val_ds,\n",
        "        latent_dim=latent_dim,\n",
        "        label=[0.3, 0.9, 1.0],\n",
        "        save_dir=run_path,\n",
        "        csv_log_path=os.path.join(run_path, \"qpo_scores.csv\")\n",
        "    )\n",
        "\n",
        "    val_logger = ValidationLossLogger(\n",
        "        val_dataset=val_ds,\n",
        "        latent_dim=latent_dim,\n",
        "        generator=generator,\n",
        "        discriminator=discriminator,\n",
        "        save_dir=run_path,\n",
        "        csv_path=os.path.join(run_path, \"val_losses.csv\")\n",
        "    )\n",
        "\n",
        "    csv_logger = tf.keras.callbacks.CSVLogger(\n",
        "        os.path.join(run_path, \"training_log.csv\"))\n",
        "\n",
        "    # Train\n",
        "    gan.fit(\n",
        "        train_ds,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[monitor, val_logger, csv_logger]\n",
        "    )\n",
        "\n",
        "    # Save final models\n",
        "    generator.save(os.path.join(run_path, \"generator_final.keras\"))\n",
        "    discriminator.save(os.path.join(run_path, \"discriminator_final.keras\"))\n",
        "\n",
        "    print(f\"Completed {config_name}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u0vcvmJLQ3NW",
      "metadata": {
        "id": "u0vcvmJLQ3NW"
      },
      "outputs": [],
      "source": [
        "# !zip -r /content/drive/MyDrive/saved_models/gan_outputs.zip /content/gan_outputs/\n",
        "!zip -r /content/drive/MyDrive/saved_models/params.zip /content/cgan_hyperparam_runs/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
